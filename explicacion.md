

# uv run main la camara de aca 
# uv run main  --dataset test2 corre el video del pasto (hablar con luca manana)


# Visi√≥n general r√°pida
**Qu√© ves en pantalla:** el visor 3D (OpenGL + pygame) dibuja:
- una **franja de c√°maras** (frustums) que representan las poses estimadas (roja la actual, verdes las pasadas)
- una **polil√≠nea** azul con la trayectoria (traducciones acumuladas)
- adem√°s, hay un visor 2D donde se pintan los **matches de features** entre cuadros consecutivos.

**Flujo de datos de un frame:**
1) `main.py` carga config y dataset ‚Üí crea `Camera` y un `DataLoader` (im√°genes o video).
2) En el bucle: redimensiona el frame y llama a `vo.process_frame(img, i)`.
3) `VisualOdometry` detecta y describe features (ORB), matchea contra el frame previo, estima **R** y **t** (matriz esencial) y **acumula la pose**.
4) `Display2D` pinta matches en el frame; `Display3D` recibe `poses` y `translations` y actualiza el mapa 3D.

---

# `camera.py`
**Prop√≥sito:** encapsular par√°metros intr√≠nsecos de c√°mara y convertir puntos entre coordenadas de pixel y **coordenadas normalizadas** de c√°mara.

- `Camera.__init__(width, height, fx, fy, cx, cy)`
  - Guarda dims y par√°metros intr√≠nsecos.
  - Construye la matriz **K** = \[ [fx, 0, cx], [0, fy, cy], [0, 0, 1] ] y su inversa `Kinv`.
  - `pp = (cx, cy)` es el **punto principal**.

- `__str__`: imprime `K` conveniente para debug.

- `normalize_pts(self, pts)`: **No implementado**. El comentario dice *‚Äú3D world ‚Üí 2D image‚Äù*, pero no se usa.

- `denormalize_pts(self, pts)`
  - **OJO con el nombre/comentario:** el comentario dice *‚Äú2D image ‚Üí 3D world‚Äù*, pero **lo que hace realmente** es: toma puntos **en pixel** `(u, v)`, les a√±ade 1 para homog√©neas y aplica `Kinv` ‚Üí devuelve **coordenadas normalizadas** `(x', y')` en el plano z=1 de la c√°mara.
  - Se usa antes de estimar la **matriz esencial** con focal=1 y pp=(0,0).

> üí° Mejora sugerida: renombrar a `pixels_to_normalized()` y corregir comentarios.

---

# `dataset.py`
**Prop√≥sito:** lectura de datos (im√°genes o video) como iteradores Python.

- `DataLoader(path)`
  - Clase base: valida que `path` exista. Define interfaz con `__iter__` y `__next__` abstractos.

- `ImageLoader(path)`
  - Lista los archivos **.png** (ordenados) dentro de `path` y los mete en un `deque`.
  - **Asume** que hay im√°genes y toma la **√∫ltima** para obtener `height`, `width`.
  - `__next__`: hace `cv2.imread()` del siguiente PNG; levanta `StopIteration` cuando se acab√≥.
  - **Detalle:** s√≥lo acepta `.png`. Si tu dataset es `.jpg`, no lo va a leer.

- `VideoLoader(path)`
  - Usa `cv2.VideoCapture` para leer MP4.
  - Expone `width` y `height` del video.
  - `__next__`: lee un frame; si no hay m√°s, `StopIteration`. Convierte BGR‚ÜíRGB.

> üí° Mejora sugerida: permitir extensiones m√∫ltiples (`.jpg`, `.jpeg`) y ordenar por timestamp si hace falta.

---

# `display2d.py`
**Prop√≥sito:** mostrar el frame actual con matches dibujados.

- Inicializa `pygame`, crea `screen` y una `surface` del mismo tama√±o.
- `paint(img)`: procesa eventos para que la ventana no se cuelgue, copia el array a la surface y hace `flip()`.

> Nota: espera **arrays RGB** con shape `(H, W, 3)`.

---

# `display3d.py`
**Prop√≥sito:** visor 3D en **otro proceso** (multiprocessing) que renderiza frustums y trayectoria.

- `VisualOdometryState`: contenedor ligero con `poses` (N√ó4√ó4) y `translations` (N√ó3).
- `Display.__init__()`
  - Crea una `Queue()` y un `Process` (`viewer_thread`) que corre el loop de render.
  - Esto evita que el render 3D **bloquee** el hilo principal.

- `viewer_thread(q)`
  - Configura ventana (1024√ó550), inicia loop de eventos y refresco a 60 FPS.
  - `viewer_refresh(q)`: consume el √∫ltimo estado de la cola, actualiza c√°mara 3D y dibuja.

- `viewer_init(w, h)`
  - Setea OpenGL: depth test, proyecci√≥n `gluPerspective(45¬∞, aspect, 0.1, 5000)`.
  - Define una c√°mara virtual (`gluLookAt`) con posici√≥n inicial (`camera_*`) y target `(0,0,0)`.

- `draw_camera_frustum(pose, scale)`
  - Dibuja un frustum de c√°mara aplicando la **pose 4√ó4** (usa `glMultMatrixf(pose.T.flatten())`).

- `draw_line_strip(points)`
  - Dibuja una **l√≠nea** conectando la secuencia de `translations`.

- `update(vo)`
  - El hilo principal empaqueta `vo.poses` y `vo.translations` en un `VisualOdometryState` y lo mete en la cola.

> üí° Tips: no hay controles de usuario (rotar/orbitar). La c√°mara "persigue" la √∫ltima pose moviendo `camera_x/y/z` en funci√≥n de `current_pose[:3,3]`.

---

# `features.py`
**Prop√≥sito:** detecci√≥n, descripci√≥n y **matching** de puntos de inter√©s.

- `FeatureManager.__init__()` crea un extractor ORB.
- `detect(frame)`
  - Usa **Shi‚ÄìTomasi** (`goodFeaturesToTrack`) para detectar hasta 2000 esquinas.
  - Las convierte a `cv2.KeyPoint` (size=5) para que ORB pueda describirlas.
  - **Recrea** el extractor ORB cada vez (no hace falta, pero no duele).

- `compute(frame, kps)`
  - Llama `ORB.compute` para obtener descriptores binarios; guarda `self.kps`, `self.des`.

- `detect_and_compute(frame)` = detect + compute.

- `get_matches(cur_kps, ref_kps, cur_des, ref_des)`
  - `BFMatcher(NORM_HAMMING).knnMatch(ref_des, cur_des, k=2)` y **ratio test de Lowe** (0.75).
  - `assert len(good) > 8` para garantizar m√≠nimo de pares para pose (si falla, se cae).
  - Devuelve `ref_pts` y `cur_pts` (arrays Nx2) **en pixeles** de los keypoints matcheados.

> üí° Mejora sugerida: manejar el caso de pocos matches sin `assert` (p. ej. saltar frame o relajar umbral).

---

# `main.py`
**Prop√≥sito:** orquestaci√≥n.

- `parse_cfg('config.yaml', dataset)` lee YAML y devuelve el bloque de ese dataset (`fx, fy, cx, cy, w, h, path`).
- Crea `Camera(w, h, fx, fy, cx, cy)`.
- Seg√∫n `path` sea carpeta o archivo: `ImageLoader` o `VideoLoader`.
- `_main(camera, loader)`
  - Inicializa `Display2D` (w√óh) y `Display3D`.
  - Crea `VisualOdometry(camera)`.
  - Prepara un lienzo 800√ó800 para el mapa 2D (`mapd`).
  - Loop sobre frames: `vo.process_frame(img, i)`.
  - Dibuja la imagen con matches (`display2D.paint(vo.draw_img)`).
  - A partir del frame 2, proyecta `vo.translations[-1]` a coordenadas del mapa 2D con `fit_point` y va pintando c√≠rculos + texto (x,y).
  - Llama `display3D.update(vo)` en cada iteraci√≥n para refrescar el visor 3D.
  - Al final, `cv2.destroyAllWindows()`.

- `fit_point((tw, th), (x, y, z))`
  - Mapea **x y z** de la traducci√≥n a un lienzo 2D (ignora y). Centra en `w//2` y baja el origen vertical.

> üí° Detalle: se hace `cv2.resize(img, (width, height))` para que el procesamiento use las mismas dims que `Camera`.

---

# `odometry.py`
**Prop√≥sito:** VO (Visual Odometry) monocular muy b√°sica: estima **R** y **t** entre frames consecutivos y acumula.

- Estados principales:
  - `cur_img`, `ref_img`: imagen actual y de referencia (gris)
  - `cur_kps/des`, `ref_kps/des`: keypoints y descriptores ORB
  - `cur_matched_kps/ref_matched_kps`: puntos matcheados (pixeles)
  - `cur_R` (3√ó3), `cur_t` (3√ó1): rotaci√≥n y traslaci√≥n **acumuladas**
  - `translations`: lista de `t` acumuladas; `poses`: lista de matrices 4√ó4 (`pose_Rt`)

- `process_frame(img, frame_id)`
  1) Convierte a gris si hace falta.
  2) Detecta y describe features.
  3) Si es el primer frame (id=0): s√≥lo prepara `draw_img`.
  4) Si no: hace matching contra el frame previo y llama a `estimate_pose()`.
     - Filtra `mask` de inliers que devuelve `recoverPose`.
     - Dibuja matches (`draw_features`).
     - **Actualiza** pose acumulada:
       - `cur_t = cur_t + scale * cur_R.dot(t)` ‚Üí compone traslaci√≥n en el marco actual (usa un **escala** fija `0.8`).
       - `cur_R = cur_R.dot(R)` ‚Üí compone rotaci√≥n.
     - Agrega a `translations` y `poses`.
  5) Actualiza referencias para el siguiente frame.

- `estimate_pose(use_fundamental_matrix=False)`
  - Por defecto usa **Matriz Esencial** `E` (no `F`), porque primero transforma pixeles a **coordenadas normalizadas** con `Camera.denormalize_pts`.
  - `cv2.findEssentialMat(cur, ref, focal=1, pp=(0,0), RANSAC, prob=0.999, threshold=0.003)`
  - `cv2.recoverPose(E, cur, ref, focal=1, pp=(0,0))` ‚Üí devuelve R, t y m√°scara de inliers.
  - Convierte la m√°scara a √≠ndices y los retorna junto a (R, t).

- `draw_features(img)`
  - Dibuja en azul los puntos y en verde las l√≠neas entre pares (ref‚Üícur). Devuelve un RGB para el visor 2D.

> ‚ö†Ô∏è Limitaciones importantes:
> - **Escala desconocida:** Monocular VO recupera t **hasta un factor de escala**. Aqu√≠ usan `self.scale = 0.8` fija ‚Üí la distancia absoluta es arbitraria.
> - **Fragilidad a pocos matches:** `assert len(good) > 8` puede tirar el programa; conviene manejarlo con `if`.
> - **Sin bundle adjustment / optimizaci√≥n global:** la deriva crecer√° con el tiempo.

---

# `util.py`
- `add_ones(x)`: a√±ade el 1 homog√©neo (a un punto o a una lista de puntos).
- `pose_Rt(R, t)`: empaqueta a **matriz 4√ó4** con R en la esquina y t en la √∫ltima columna.

---

# Cosas a tener en cuenta / Debug r√°pido
- **Config correcta:** `config.yaml` debe tener `fx, fy, cx, cy, w, h` consistentes con tu fuente (video o im√°genes). Si descoordinan, los matches e inliers se vuelven pobres.
- **Extensiones:** `ImageLoader` s√≥lo toma `.png`. Si tus datos son `.jpg`, renombra o ajusta el c√≥digo.
- **Rendimiento:** ORB con 2000 features por frame est√° OK, pero puedes bajar a 1000 si tu CPU sufre.
- **Cierre del visor:** presion√° `Esc` en la ventana 3D para salir limpio (ver√°s ‚ÄúQuitting viewer‚Ä¶‚Äù en la terminal).

---

# Mejoras f√°ciles (si quer√©s iterar)
1) **Nombres claros en `Camera`:** renombrar `denormalize_pts` a `pixels_to_normalized` y arreglar comentarios.
2) **Manejar pocos matches:** si `len(good) <= 8`, saltar actualizaci√≥n de pose y mantener la previa.
3) **Escala din√°mica:** estimar escala comparando parallax promedio o usando datos de odometr√≠a externa si existiera; si es secuencia KITTI, podr√≠as comparar con odometr√≠a ground-truth para calibrar un factor.
4) **Soporte `.jpg`:** extender el filtro en `ImageLoader`.
5) **Threshold adaptativo:** en `findEssentialMat` usar un umbral basado en el **ruido** esperado (p. ej., proporcional a 1/focal o al tama√±o de imagen).

---

# Mini checklist para reproducir lo que ves
- `uv run main --dataset=mi_video` (tras agregar bloque en `config.yaml` con la ruta a tu MP4)
- Confirm√° que el visor 2D muestra **l√≠neas verdes** entre features ‚Üí significa que el matching funcion√≥.
- En el 3D, la franja roja (pose actual) se desplaza; la azul (trayectoria) crece.

Si quer√©s, en la pr√≥xima vuelta te dejo **comentado en l√≠nea** cada archivo con docstrings y `#` explicativos para que lo pegues directo en tu repo.

